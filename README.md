# LIGHT: Multi-Modal Text Linking for Historical Maps
This repository provides training and inference scripts for pretraining and fine-tuning the **LIGHT** model, which integrates polygon geometry and visual-textual features for text linking on historical maps.

## üìö Pretraining

### 1. Polygon Encoder Pretraining

```bash
CUDA_VISIBLE_DEVICES="0" torchrun \
  --nproc_per_node=1 \
  --nnodes=1 \
  --node_rank=0 \
  --master_addr=127.0.0.1 \
  --master_port=14476 \
  pretrain_poly.py --config configs/pretrain_poly.yaml
```

### 2. Full LIGHT Model Pretraining

```bash
CUDA_VISIBLE_DEVICES="0" torchrun \
  --nproc_per_node=1 \
  --nnodes=1 \
  --node_rank=0 \
  --master_addr=127.0.0.1 \
  --master_port=14476 \
  pretrain.py --config configs/pretrain_light.yaml
```

## üîß Fine-Tuning

```bash
python train.py --config configs/light.yaml
```

## üîç Inference

```bash
python inference.py \
  --test_dataset MapText_test \
  --out_file predict.json \
  --model_dir ./_weights/finetune_light \
  --anno_path /home/yaoyi/shared/spotter-data/icdar-maptext/competition-data/icdar24-test-png-annotations.json \
  --img_dir /home/yaoyi/shared/spotter-data/icdar-maptext/competition-data/icdar24-test-png/test_images/
```

## üìÅ Notes

- Update the paths to your dataset, annotations, and trained model checkpoints as needed.
- All the data configurations are in the dataset/buildin.py
- All configs can be modified under the `configs/` directory to adjust hyperparameters or dataset settings.

## üîó References

If you find this repository useful in your own work, we would appreciate a citation to the accompanying paper:

```bibtex
@inproceedings{ weinman2024counting,
   authors = {Lin, Yijun and Olson, Rhett and Wu, Junhan and Chiang, Yao-Yi and Weinman, Jerod},
   title = {LIGHT: Multi-Modal Text Linking on Historical Maps},
   booktitle = {19th International Conference on Document Analysis and Recognition ({ICDAR} 2025)},
   series = {Lecture Notes in Computer Science},
   publisher = {Springer},
   location = {Wuhan, China},
   year = {2025}
}
```

